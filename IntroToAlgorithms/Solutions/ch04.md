# Divide and Conquer (Conceptual Exercises and Solutions)

## The maximum-subarray problem

**4.1-1**

What does `FIND-MAXIMUM-SUBARRAY` return when all elements of A are negative?

> It returns the largest number of em' all.

**4.1-2**

Write pseudocode for the brute-force method of solving the maximum-subarray problem. Your procedure should run in $\Theta(n^2)$ time.

```pascal
fmsbf(A, n)
    max = -INFINITY
    for i = 1 to n
        do sum = 0
           for j = i to n
               do sum = sum + A[j]
                  if sum > max
                      do max = sum
                         low = i
                         high = j

    return (low, high, max)        
```

**4.1-3**

Implement both the brute-force and recursive algorithms for the maximum subarray problem on your own computer. What problem size $n_0$ gives the crossover point at which the recursive algorithm beats the brute-force algorithm? Then, change the base case of the recursive algorithm to use the brute-force algorithm whenever the problem size is less than $n_0.$ Does that change the crossover point?

> My machine keeps giving `0ms` for input sizes upto 800 for the brute force method, which either means I'm calculating the time wrong or something weird is happening. It is also possible that I'm in possession of a supercomputer because I initially assumed that $n_0$ would be 40-50. I'm skipping this question for now.

**4.1-4**

Suppose we change the definition of the maximum-subarray problem to allow the result to be an empty subarray, where the sum of the values of an empty subarray is 0. How would you change any of the algorithms that do not allow empty subarrays to permit an empty subarray to be the result?

> If the answer is negative, it will return an empty subarray. This is analogous to not participating in trading if we know in advance that the stock price will only fall.

**4.1-5**

Use the following ideas to develop a nonrecursive, linear-time algorithm for the maximum-subarray problem. Start at the left end of the array, and progress toward the right, keeping track of the maximum subarray seen so far. Knowing a maximum subarray of $A[1,\dots,j],$ extend the answer to find a maximum subarray ending at index $j+1$ by using the following observation: a maximum subarray of $A[1,\dots,j+1]$ is either a maximum subarray of $A[1,\dots,j],$ or a subarray $A[i,\dots,j+1],$ for some $1\le i\le j+1.$ Determine a maximum subarray of the form $A[i,\dots,j+1]$ in constant time based on knowing a maximum subarray ending at index $j.$

```pascal
fmsl(A, low, high)
    left = 0
    right = 0
    temp = 0
    sum = a[low]
    for i = low to high
        temp = temp + a[i]
        if temp > sum
            sum = temp
            right = i
        if temp < a[i]
            left = right = i
            temp = sum = a[i]

    return (left, right, sum)
```

## Strassen’s algorithm for matrix multiplication

**4.2-1**

Use Strassen’s algorithm to compute the matrix product $$\begin{pmatrix}1 & 3\\7 & 5\end{pmatrix}\begin{pmatrix}6 & 8\\4 & 2\end{pmatrix}.$$ Show your work.

> Too long to type it out, but it is pretty straightforward.

**4.2-2**

Write pseudocode for Strassen’s algorithm.

```pascal
strassen(a, b)
    // split a, b into a11, a12, a21, a22, b11, b12, b21 and b22
    s1 = b12 - b22
    s2 = a11 + a12
    s3 = a21 + a22
    s4 = b21 - b11
    s5 = a11 + a22
    s6 = b11 + b22
    s7 = a12 - a22
    s8 = b21 + b22
    s9 = a11 - a21
    s10 = b11 + b12

    p1 = a11 * s1
    p2 = s2 * b22
    p3 = s3 * b11
    p4 = a22 * s4
    p5 = s5 * s6
    p6 = s7 * s8
    p7 = s9 * s10

    c11 = p5 + p4 - p2 + p6
    c12 = p1 + p2
    c21 = p3 + p4
    c22 = p5 + p1 - p3 - p7

    // combine c11, c12, c21, c22 into c

    return c
```

**4.2-3**

How would you modify Strassen’s algorithm to multiply $n \times n$ matrices in which $n$ is not an exact power of 2? Show that the resulting algorithm runs in time $\Theta(n^{\lg7}).$

> Pad the matrices with zeroes until the next $n$ is reached. This will still work in $\Theta(n^{\lg7})$ asymptotically.

**4.2-4**

What is the largest $k$ such that if you can multiply $3\times3$ matrices using $k$ multiplications (not assuming commutativity of multiplication), then you can multiply $n\times n$ matrices in time $o(n^{\lg7})?$ What would the running time of this algorithm be?

> If $n$ is a power of 3, the recursion time will be $T(n)=kT(n/3)+O(1).$ By the master theorem, $\log_3k<\lg7\implies k=3^{\lg7}=22.$

**4.2-5**

V. Pan has discovered a way of multiplying $68\times68$ matrices using 132,464 multiplications, a way of multiplying $70\times70$ matrices using 143,640 multiplications, and a way of multiplying $72\times72$ matrices using 155,424 multiplications. Which method yields the best asymptotic running time when used in a divide-and-conquer matrix-multiplication algorithm? How does it compare to Strassen’s algorithm?

> $\log_{68}132464=2.795128$
>
> $\log_{70}143640=2.795123$
>
> $\log_{72}155424=2.795147$
>
> The fastest is $70\times70$ using 143640 multiplications (asymptotically).

**4.2-6**

How quickly can you multiply a $kn\times n$ matrix by an $n\times kn$ matrix, using Strassen’s algorithm as a subroutine? Answer the same question with the order of the input matrices reversed.

> First case: $\Theta(k^2n^{\lg7})$
>
> Second case: $\Theta(kn^{\lg7})$

**4.2-7**

Show how to multiply the complex numbers $a+bi$ and $c+di$ using only three multiplications of real numbers. The algorithm should take a, b, c, and d as input and produce the real component $ac+bd$ and the imaginary component $ad+bc$ separately.

> 1: $(a+b)(c+d)=ac+ad+bc+bd$ \
> 2: $a\cdot c$ \
> 3: $b\cdot d$
>
> Real part = 2 - 3 \
> Imaginary part = 1 - 2 - 3
