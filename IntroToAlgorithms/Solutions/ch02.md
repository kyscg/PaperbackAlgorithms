# Getting Started (Conceptual Exercises and Solutions)

## Insertion Sort

**2.1-1**

Illustrate the operation of `INSERTION-SORT` on the array A = <31, 41, 59, 26, 41, 58>.

> The numbers in bold will indicate the position of the key and the italics will show the numbers against which the key will be compared.

A = [*31*, *41*, *59*, **26**, 41, 58]

A = [26, 31, 41, *59*, **41**, 58]

A = [26, 31, 41, 41, *59*, **58**]

A = [26, 31, 41, 41, 58, 59]

> Note that in the second iteration, 41 is not compared to the other 41 because Insertion Sort is a stable sorting algorithm.

**2.1-2**

Rewrite the `INSERTION-SORT` procedure to sort into non-increasing instead of non-decreasing order.

> We just invert the key check with the previous elements.

```pascal
for i = 2 to length(A)
    do key = A[i]
    //Insert A[i] in sorted sequence A[i .. i - 1]
       j = i - 1
       while j > 0 and A[j] < key
       do A[j + 1] = A[j]
          j = j - 1
       A[j + 1] = key
```

**2.1-3**

Consider the _**searching problem**_:

**Input**: A sequence of `n` numbers `A = a1,a2,...,an` and a value `v`.

**Output**: An index `i` such that `v = A[i]` or the special value `NIL` if `v` does not appear in `A`.

Write pseudocode for **linear search**, which scans through the sequence, looking for `v`. Using a loop invariant, prove that your algorithm is correct. Make sure that your loop invariant fulfills the three necessary properties.

```pascal
for i = 1 to length(A)
   if A[i] == v
      return i

return NIL
```

> Initialization: The subarray is empty and hence doesn't contain v.
>
> Maintenance: For every iteration, if `A[i]` equals `v`, we move to termination, or we go back to initialization with a different subarray.
>
> The loop terminates when we find `v` or when we reach end of loop.

**2.1-4**

Consider the problem of adding two n-bit binary integers, stored in two n-element arrays A and B. The sum of the two integers should be stored in binary form in an (n + 1)-element array C. State the problem formally and write pseudocode for adding the two integers.

> **Input**: Two n-bit arrays [a1, a2,..., an], [b1, b2,..., bn] which are binary representations of A and B.
>
> **Output**: One n+1-bit array [c1, c2,..., cn] which is a binary representation of C : C = A + B.

```pascal
for i = 1 to length(A) + 1
   do C[i] = 0

next = 0

for i = length(A) to 1
   do next = (A[i] + B[i] + C[i])
      C[i] = next % 2
      C[i + 1] = next / 2
```

---

## Analyzing Algorithms

**2.2-1**

Express the function $\frac{n^3}{1000} − 100n^2 − 100n + 3$ in terms of $\Theta$-notation.

> $n^3/1000 − 100n^2 − 100n + 3 \approx n^3 = \Theta(n^3)$

**2.2-2**

Consider sorting `n` numbers stored in array `A` by first finding the smallest element of `A` and exchanging it with the element in `A[1]`. Then find the second smallest element of `A`, and exchange it with `A[2]`. Continue in this manner for the first `n − 1` elements of `A`. Write pseudocode for this algorithm, which is known as **_selection sort_**. What loop invariant does this algorithm maintain? Why does it need to run for only the first `n − 1` elements, rather than for all `n` elements? Give the best-case and worst-case running times of selection sort in -notation.

```pascal
for i = 1 to length(A) - 1
   do key = i
      for j = i + 1 to length(A)
         if A[j] < A[key]
            do key = j
      temp = A[key]
      A[key] = A[i]
      A[i] = temp
```

> The loop invariant that is maintained is that for every iteration `i`, the subarray `A[1,..., i - 1]` will be sorted in increasing order.
>
> From the above loop invariant, at the n<sup>th</sup> iteration, `A[1,..., n - 1]` is already sorted **into its place** implying that the element in the last place needn't be sorted.
>
>
> Irrespective of the input, the algorithm performs checks on every key against the other keys, therefore:
>
> Best-case running time: $\Theta(n^2)$ \
> Worst-case running time: $\Theta(n^2)$

**2.2-3**

Consider linear search again (see Exercise 2.1-3). How many elements of the input sequence need to be checked on the average, assuming that the element being searched for is equally likely to be any element in the array? How about in the worst case? What are the average-case and worst-case running times of linear search in $\Theta$-notation? Justify your answers.

> On average, $n/2$ inputs need to be checked and in the worst case, all $n$ inputs need to searched. The worst case scenario is pretty straightforward. On average means that we fill up the array with n random numbers, in which case there will probably be an equal number of keys which are greater and smaller than our number i.e, our number will be right in the middle on average. **Note that the text also states that it isn't easy to specify what the average means in general while doing analysis**.
>
> It follows from the above explanation that both average and worst case running times are $\Theta(n).$

**2.2-4**

How can we modify almost any algorithm to have a good best-case running time?

> One thing we can do is to remove loops, this usually means that instead of checking every key. We just check whether the input is a special case.
>
> For example, for any sort, you can make best case $\Theta(n)$ by checking if the array is already sorted - and if it is, return it as it is.
---

## Designing Algorithms

**2.3-1**

Using Figure 2.4 as a model, illustrate the operation of merge sort on the array A = [3, 41, 52, 26, 38, 57, 9, 49]

```css
[3 41 52 26 38 57 9 49]

// divide
[3 41 52 26] [38 57 9 49]

// divide
[3 41] [52 26] [38 57] [9 49]

// sort
[3 41] [26 52] [38 57] [9 49]

// merge
[3 26 41 52] [9 38 49 57]

// merge
[3 9 26 38 41 49 52 57]
```

**2.3-2**

Rewrite the `MERGE` procedure so that it does not use sentinels, instead stopping once either array `L` or `R` has had all its elements copied back to `A` and then copying the remainder of the other array back into `A`.

```pascal
MERGE(A, p, q, r)
   n1 = q - p + 1
   n2 = r - q
   
   //create arrays L[n1 + 1] and R[n2 + 1]

   for i = 1 to n1
      do L[i] = A[p + i - 1]
   for j = 1 to n2
      do R[i] = A[q + j]
   
   i = 1
   j = 1

   for k = p to r
      do if i > n1
            A[k] = R[j]
            j = j + 1
         else if j > n2
            A[k] = L[i]
            i = i + 1
         else if L[i] <= R[j]
            A[k] = L[i]
            i = i + 1
         else
            A[k] = R[j]
            j = j + 1
```

**2.3-3**

Use mathematical induction to show that when n is an exact power of 2, the solution of the recurrence

$$T(n) = 2 \text{ if } n=2,$$
$$T(n) = 2T(n/2) + n \text{ if } n = 2^k, \text{ for } k > 1.$$

is $T(n) = n \lg{n}.$

> For $k=1,n=2, T(n) = 2$ which is equal to $2 \lg{2}.$
>
> If $k = k,\text{ } T(2^k) = 2^k(k)$ \
> Now take $k = k + 1$ \
> $T(2^{k + 1}) = 2T(2^k) + 2^{k + 1}$ \
> $T(2^{k + 1}) = 2\cdot 2^k(k) + 2^{k + 1}$ \
> $T(2^{k + 1}) = (2^{k + 1})(k + 1)$ $\blacksquare$

**2.3-4**

Insertion sort can be expressed as a recursive procedure as follows. In order to sort `A[1,...,n]`, we recursively sort `A[1,...,n − 1]` and then insert `A[n]` into the sorted array `A[1,...,n − 1]`. Write a recurrence for the running time of this recursive version of insertion sort.

> Step 1: Sort the `[n - 1]` elements \
> Step 2: Insert `A[n]` in place
>
> It is trivial to notice that the first step takes $\Theta(n - 1)$ time. The second step takes $c(\frac{n - 1}{2})$ time, we need to notice here that when $n > 1,$ insertion takes $c(\frac{n}{2})$ time on average as we need to shift half the elements.
>
> $$T(n) = T(n - 1) + c\bigg(\frac{n-1}{2}\bigg)$$
> $$T(n) = T(n - 2) + c\bigg(\frac{n-2}{2}\bigg) + c\bigg(\frac{n-1}{2}\bigg)$$
> $$\ddots$$
> $$T(n) = A + B\bigg(\frac{n(n-1)}{2}\bigg) + C(n-1)$$
> $$T(n) = \Theta(n^2)$$

**2.3-5**

Referring back to the searching problem (see Exercise 2.1-3), observe that if the sequence A is sorted, we can check the midpoint of the sequence against v and eliminate half of the sequence from further  consideration. **_Binary search_** is an algorithm that repeats this procedure, halving the size of the remaining portion of the sequence each time. Write pseudocode, either iterative or recursive, for binary search. Argue that the worst-case running time of binary search is $\Theta(\lg{n}).$

```pascal
BinSearch(A, p, r)
   q = (p + r) / 2
   if v = A[q]
      do return q
   else if v > A[q]
      do BinSearch(A, q + 1, r)
   else
      do BinSearch(A, p, q - 1)
```

> Worst case is when `v` is not there in the array implying that the time complexity will be $\Theta(\lg{n}).$ The reason for this is that we will need to keep halving the array.
>
> The recurrence formula is $T(n) = 2T(\frac{n-1}{2}) + \Theta(1).$

**2.3-6**

Observe that the **while** loop of lines 5 – 7 of the `INSERTION-SORT` procedure in Section 2.1 uses a linear search to scan (backward) through the sorted subarray `A[1,...,j − 1]`. Can we use a binary search (see Exercise 2.3-5) instead to improve the overall worst-case running time of insertion sort to $\Theta(n\lg{n})$?

> No, the major costs there are both the searching as well as the shifting of keys to accomodate the current one. Worst case running time will therefore remain the same even if we optimize the search. $\Theta(n) + \Theta(\lg{n})=\Theta(n).$

**2.3-7**

Describe a $\Theta(n\lg{n})$-time algorithm that, given a set `S` of `n` integers and another integer `x`, determines whether or not there exist two elements in `S` whose sum is exactly `x`.

> We could sort in $\Theta(n\lg{n})$ by using merge sort and then use two pointers coming in from the two ends of the array. The idea is that the sum of keys at the two pointers should equal `x`. See pseudocode below.

```pascal
MergeSort(A, 1, n)
l = 0
r = n - 1
while l < r
   do if A[l] + A[r] = v
         then return True
      else if A[l] + A[r] > v
         then r = r - 1
      else
         l = l + 1

return False
```

> Time complexity: $\Theta(n\lg{n}) + \Theta(n)$

---

## Problems

**2-1 _Insertion sort on small arrays in merge sort_**

Although merge sort runs in $\Theta(n\lg{n})$ worst-case time and insertion sort runs in $\Theta(n^2)$ worst-case time, the constant factors in insertion sort make it faster for small $n.$ Thus, it makes sense to use insertion sort within merge sort when subproblems become sufficiently small. Consider a modification to merge sort in which $n/k$ sublists of length $k$ are sorted using insertion sort and then merged using the standard merging mechanism, where $k$ is a value to be determined.

**_a._** Show that the $n/k$ sublists, each of length $k,$ can be sorted by insertion sort in $\Theta(nk)$ worst-case time.

> If $k=1,$ it is just a linear traversal and hence the worst case time is $\Theta(n).$
>
> For sublist of length $k,$ we have $T(n) = k(k - 1)$ and for $n/k$ sublists, $T(n) = (n/k)\cdot k(k-1) = \Theta(nk).$

**_b._** Show that the sublists can be merged in $\Theta(n \lg{n/k})$ worst-case time.

> If the number of sublists is $n/k,$ we will need $\lg{n/k}$ merges (we count a merge everytime we join $n/k$ subarrays into $n/2k$.) In the worst case scenario, cost of performing the merge across all subarrays is $n.$ $$\therefore T(n) = \Theta(n\lg{n/k})$$

**_c._** Given that the modified algorithm runs in $\Theta(nk + n \lg{(n/k)})$ worst-case time, what is the largest asymptotic ($\Theta$-notation) value of $k$ as a function of $n$ for which the modified algorithm has the same asymptotic running time as standard merge sort?

> $k = \lg{n}$

**_d._** How should $k$ be chosen in practice?

> In practice, something close to the above value should work well but a more thorough analysis like we did in the previous chapter might be optimal.

**_2-2 Correctness of bubblesort_**

Bubblesort is a popular sorting algorithm. It works by repeatedly swapping adjacent elements that are out of order.

```pascal
for i = 1 to length[A]
   do for j = length[A] downto i + 1
      do if A[j] < A[j - 1]
         then swap A[j], A[j - 1]
```

**_a._** Let `A'` denote the output of `BUBBLESORT(A)`. To prove that `BUBBLESORT` is correct, we need to prove that it terminates and that

`A'[1] ≤ A'[2] ≤ ... ≤ A'[n]`,

where `n = length[A]`. What else must be proved to show that `BUBBLESORT` actually sorts?

> This question has to be the best one yet, I didn't get the answer for a long while, but it turns out that in the original unsorted array, we need to have the elements `A'[1], A'[2],..., A'[n]`. Pretty smart.

**_b._** State precisely a loop invariant for the **for** loop in lines 2–4, and prove that this loop invariant holds.

> The loop invariant is that at the beginning of every iteration, the position of the smallest element in `A[1,...,n]` is atmost `j`.
>
> * Initialization: All elements are in `[1,...,length(A)]`. So, the smallest definitely is.
>
> * Maintenance: At every iteration, the smallest element "bubbles" down to the `i`<sup>th</sup> position.
>
> * Termination: Upon termination, the smallest key in `A[1,...,n]` is at position `i`.

**_c._** Using the termination condition of the loop invariant proved in part (b), state a loop invariant for the **for** loop in lines 1–4 that will allow you to prove the in-equality in question.

> The loop invariant is that after every iteration, the array `A[1,...,i]` is in sorted order.
>
> * Initialization: There is only one key, trivially sorted.
>
> * Maintenance: Using the loop invariant from the previous part, the smallest element on termination of the inner loop is in the `i`<sup>th</sup> position. This ensures that `A[1,...,i]` is sorted.
>
> * Termination: On reaching `i = n`, we have a sorted `A[1,...,n]`.

**_d._** What is the worst-case running time of bubblesort? How does it compare to the running time of insertion sort?

> In the worst case, we check every element and make swaps. Thus, the worst case running time is $\Theta(n^2).$ The difference with respect to insertion sort is the best case running time which is $\Theta(n^2)$ for bubble sort and $\Theta(n)$ for insertion sort.

**_2-3 Correctness of Horner’s rule_**

The following code fragment implements Horner’s rule for evaluating a polynomial$$P(x) = \sum_{k=0}^{n}a_kx^k \\ = a_0 + x(a_1 + x(a_2 + \cdots + x(a_{n-1} + xa_n)\cdots)),$$ given the coefficients $a_0, a_1, \dots,a_n$ and a value for $x:$

```pascal
y = 0
i = n
while i >= 0
   do y = a[i] + x * y
      i = i - 1
```

**_a._** What is the asymptotic running time of this code fragment for Horner’s rule?

> Pretty straightforward innit? $\Theta(n).$

**_b._** Write pseudocode to implement the naive polynomial evaluation algorithm that computes each term of the polynomial from scratch. What is the running time of this algorithm? How does it compare to Horner’s rule?

```pascal
res = 0
for i = 0 to n
   do y[i] = x
      for j = 1 to n
         do y[i] = y[i] * x
      res = res + a[i] * y[i]
```

> The algorithm above runs in $\Theta(n^2)$ time, and it is not as efficient as Horner’s rule which runs in $\Theta(n)$ time.

**_c._** Prove that the following is a loop invariant for the **while** loop in lines 3–5. At the start of each iteration of the **while** loop of lines 3–5,$$y=\sum_{k=0}^{n-(i+1)}a_{k+i+1}x^k.$$ Interpret a summation with no terms as equaling $0.$ Your proof should show that, at termination, $y=\sum_{k=0}^{n}a_{k}x^k.$

> Initially, we have $i=n,$ implying that the summation has an upper bound of -1 and thus evaluates to 0.$$y=a_i+x\sum_{k=0}^{n-(i+1)}a_{k+i+1}x^k \\ =a_i + x\sum_{k=0}^{n-i}a_{k + i}x^{k-1} \\ = \sum_{k=0}^{n-i}a_{k + i}x^k.$$ At termination, $i=0, y=\sum_{k=0}^{n}a_kx^k.$ 

**_d._** What is the worst-case running time of bubblesort? How does it compare to the running time of insertion sort?

In the previous part, we've shown that the loop terminates by returning the correct polynomial. $\text{QED.}$

**_2-4 Inversions_**

Let `A[1,...,n]` be an array of $n$ distinct numbers. If $i < j$ and `A[i] > A[j]`, then the pair $(i, j)$ is called an inversion of `A`.

**_a._** List the five inversions of the array `[2, 3, 8, 6, 1]`.

> (2, 1) (3, 1) (8, 6) (8, 1) (8, 6)

**_b._** What array with elements from the set $\{1,2,\dots,n\}$ has the most inversions? How many does it have?

> $\{n,n-1,\dots,1\}$ has $\frac{(n-1)n}{2}$ inversions.

**_c._** What is the relationship between the running time of insertion sort and the number of inversions in the input array? Justify your answer.

> It is a factor times the number of inversions. We can look at it like the number of swaps/inserts needed to resolve the inversions.

**_d._** Give an algorithm that determines the number of inversions in any permutation on $n$ elements in $\Theta(n\lg{n})$ worst-case time. (*Hint*: Modify merge sort.)

```pascal
countInv(A, p, r)
   q = (p + r) / 2
   ans = 0
   ans += countInv(A, p, q)
   ans += countInv(A, q + 1, r)
   ans += merge(A, p, q, r)
   return ans

merge(A, p, q, r)
   //create L and R
   i = 0, j = 0, ans = 0
   for k = p to r
      do if L[i] <= R[j]
         then A[k] = L[i]
              i = i + 1
         else
              ans += n1 - i + 1;
              A[k] = R[j]
              j = j + 1
   return ans
```